{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自动求导机制\n",
    "\n",
    "在tensorflow中自动求导机制非常简单，核心在于`tf.GradientTap`。我们首先会通过一个例子来给展示如何快速计算变量的梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 例子：如何计算变量的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y值：9.0\n",
      "y对x计算的梯度为：6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.Variable(initial_value=3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    # y = 3^2\n",
    "    y = tf.square(x)\n",
    "y_grad = tape.gradient(y, x)\n",
    "\n",
    "print(f\"y值：{y}\")\n",
    "print(f'y对x计算的梯度为：{y_grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于上述代码有几处知识点需要注意：\n",
    "1. 在定义Variable的时候，`initial_value`的值必须是浮点值\n",
    "1. shdi f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 例子：一元二次方程参数求解\n",
    "\n",
    "问题定义：y = x * w + b\n",
    "\n",
    "给定x和y，随机初始化参数w和b，然后通过梯度的更新来求解最优化w和b的权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 定义x和y训练数据\n",
    "x = tf.random.uniform(shape=())\n",
    "y = 2 * x + 3\n",
    "\n",
    "# 定义需要计算的相关参数\n",
    "\n",
    "w = tf.Variable(tf.random.uniform(shape=()))\n",
    "b = tf.Variable(tf.random.uniform(shape=()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [w, b]\n",
    "with tf.GradientTape() as tap:\n",
    "    y_predict = x * w + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 矩阵参数求解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source variable: w:0 -> [[1.]\n",
      " [2.]] graident: [[ 70.]\n",
      " [100.]] \n",
      "source variable: b:0 -> 1.0 graident: 30.0 \n"
     ]
    }
   ],
   "source": [
    "# 定义输出\n",
    "x = tf.constant([[1., 2.], [3., 4.]])\n",
    "y = tf.constant([[1.], [2.]])\n",
    "\n",
    "# 定义参数\n",
    "w = tf.Variable(initial_value=[[1.], [2.]], name=\"w\")\n",
    "b = tf.Variable(initial_value=1., name=\"b\")\n",
    "\n",
    "# 需要学习的参数列表\n",
    "parameters = [w, b]\n",
    "\n",
    "# 执行计算过程，同时保留梯度信息\n",
    "with tf.GradientTape() as tape:\n",
    "    loss = tf.reduce_sum(tf.square(tf.matmul(x, w) + b -y))\n",
    "\n",
    "# 计算之后的梯度信息\n",
    "parameter_gradients = tape.gradient(loss, parameters)\n",
    "\n",
    "# 最终得到的梯度信息是和参数列表一一对应的\n",
    "assert len(parameter_gradients) == len(parameters)\n",
    "\n",
    "# 循环打印变量和梯度数据\n",
    "for i in range(len(parameters)):\n",
    "    print(f\"source variable: {parameters[i].name} -> {parameters[i].numpy()} graident: {parameter_gradients[i]} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性回归计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff7a3b7848e5468f86c1583d595a1498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm, tqdm_notebook\n",
    "\n",
    "# 定义基础输入\n",
    "x = tf.random.uniform(shape=(2,3))\n",
    "y = 3 * x + 2\n",
    "\n",
    "# 定义3和2对应的参数w, b\n",
    "w = tf.Variable(initial_value=tf.random.uniform(shape=()))\n",
    "b = tf.Variable(initial_value=tf.random.uniform(shape=()))\n",
    "\n",
    "# 需要学习的参数\n",
    "parameters = [w, b]\n",
    "epochs = 1000\n",
    "# 定义优化器\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "with tqdm(total=epochs) as bar:\n",
    "    for i in range(epochs):\n",
    "        # 在tape变量作用域内是可以将所有梯度信息保留着\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_hat = x * w + b\n",
    "            loss = tf.reduce_sum(tf.square(y_hat, y))\n",
    "        # 从磁带（tape）中梯度出参数对应的梯度\n",
    "        gradients = tape.gradient(loss, parameters)\n",
    "        if i % 10 == 0:\n",
    "            bar.set_description(f\"loss: {loss}\")\n",
    "        bar.update(1)\n",
    "        # 优化器根据参数对应的梯度信息，更新参数值\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(gradients, parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
